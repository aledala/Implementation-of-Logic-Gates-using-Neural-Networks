# Implementation-of-Logic-Gates-using-Neural-Networks
We are implementing an algorithm that is used to minimize the squared error of the network output and the required output.
Using the Back Propagation Algorithm, we have successfully made the machine i.e the computer to learn & predict values for an XoR  function for any number of inputs. Errors were back propagated from each layer to the previous layer and the weights were changed so as to minimise the errors. When the output of two epochs matches, the back propagation is stopped. 
